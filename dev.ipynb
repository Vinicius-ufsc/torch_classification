{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.model import get_clip_classifier_from_pretrained\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from templates.templates import Templates\n",
    "\n",
    "template_name = 'fashion_templates'\n",
    "\n",
    "if hasattr(Templates(), template_name):\n",
    "    templates = getattr(Templates(), template_name)\n",
    "else:\n",
    "    raise Exception(f\"Template {template_name} not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path for dataset.\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "with open(Path('config') / Path('data' + '.yaml'), \"r\") as _data:\n",
    "    data = yaml.load(_data, Loader=yaml.FullLoader)\n",
    "    _data.close()\n",
    "\n",
    "class_dict = data['names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'dress', 1: 'jumpsuit', 2: 'pants', 3: 'skirt', 4: 'blouse'}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting zeroshot weights.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 29.88it/s]\n"
     ]
    }
   ],
   "source": [
    "image_classifier = get_clip_classifier_from_pretrained(clip_path = 'ViT-B/16',  # X:\\Users\\user\\.cache\\clip\\ViT-B_16.pt\n",
    "                                                  class_dict = class_dict, \n",
    "                                                  templates = templates, \n",
    "                                                  freeze_encoder= True,\n",
    "                                                  device = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features = image_classifier.val_preprocess(Image.open(r\"D:\\Datasets\\TCC\\pipe_debug\\test\\skirt\\Abstract_Brushstroke_Print_Pencil_Skirt-img_00000022.png\"))\n",
    "image_classifier.eval()\n",
    "image_features = image_features.unsqueeze(0).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = image_classifier(image_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'dress', 1: 'jumpsuit', 2: 'pants', 3: 'skirt', 4: 'blouse'}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[25.5135, 27.4516, 25.6006, 25.7649, 27.8653]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0480, 0.3335, 0.0524, 0.0617, 0.5044]], device='cuda:0',\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.nn.Softmax(dim=-1)\n",
    "m(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'blouse'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_dict[logits.argmax(dim=-1).item()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model = torch.load('zero_shot_models/vit_b_16_df1_16.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.val import val\n",
    "from core.loader import batch_loader\n",
    "\n",
    "val_data_loader = batch_loader(csv_file='/home/vinicius-cin/Datasets/DF1_16c_classification_filtered_v1/test/test.csv', \n",
    "                                        root_dir='/home/vinicius-cin/Datasets/DF1_16c_classification_filtered_v1', \n",
    "                                        transform=model.val_preprocess, \n",
    "                                        job_type = 'multiclass',\n",
    "                                        batch_size=64, num_workers=1, \n",
    "                                        is_clip=True, shuffle=False, \n",
    "                                        generator=None, worker_init_fn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ Validation metrics.\n",
    "val_results = val(model, loader = val_data_loader, criterion = None, \n",
    "                    calc_loss = False, arch = {'job_type' : 'multiclass', 'out_features' : 16}, device='cuda', metrics_conf = {'top_k':3, 'num_samples': 10})\n",
    "\n",
    "val_metrics, val_loss = val_results['metrics'], val_results['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.3400, 0.7850, 0.7200, 0.6500, 0.7100, 0.7200, 0.5600, 0.5550, 0.4150,\n",
       "         0.8350, 0.4850, 0.8250, 0.7650, 0.6150, 0.0050, 0.7600],\n",
       "        device='cuda:0'),\n",
       " tensor(0.6091, device='cuda:0'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_metrics.precision, torch.mean(val_metrics.precision)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_pipe",
   "language": "python",
   "name": "torch_pipe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
